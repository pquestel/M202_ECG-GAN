{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMeKCjrw06MrvOtVr6K7L+r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"q-yMrvmlG3pq","executionInfo":{"status":"ok","timestamp":1669055335734,"user_tz":480,"elapsed":5122,"user":{"displayName":"CENNET TUGCE TURAN","userId":"11607656021567769031"}}},"outputs":[],"source":["\n","\n","\n","from __future__ import division\n","\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","\n","from torchvision import transforms\n","from torch.autograd.variable import Variable\n","sns.set(rc={'figure.figsize':(11, 4)})\n","\n","import datetime \n","from datetime import date\n","today = date.today()\n","\n","import random\n","import json as js\n","import pickle\n","import os\n","\n","import tensorflow as tf\n","import math\n","from tensorflow.keras import layers\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-lchvFAoCyL","executionInfo":{"status":"ok","timestamp":1669055358512,"user_tz":480,"elapsed":22780,"user":{"displayName":"CENNET TUGCE TURAN","userId":"11607656021567769031"}},"outputId":"641e7ac4-70ff-4d45-9578-951f10b8255f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/My Drive/Colab Notebooks/\"\n","!pip install import-ipynb\n","import import_ipynb\n","!ls\n","\n","import data_sine\n","import sine_model\n","#from data import SineData, PD_to_Tensor\n","#from model import Generator, Discriminator , noise\n","\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","if device == 'cuda:0':\n","    print('Using GPU : ')\n","    print(torch.cuda.get_device_name(device))\n","else :\n","    print('Using CPU')"],"metadata":{"id":"oY24PzZdge4C","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1669055365045,"user_tz":480,"elapsed":6540,"user":{"displayName":"CENNET TUGCE TURAN","userId":"11607656021567769031"}},"outputId":"6b3ace81-f8de-404f-947a-3abceb9d9591"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting import-ipynb\n","  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (7.9.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.7.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 13.9 MB/s \n","\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.0.10)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.2.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->IPython->import-ipynb) (0.8.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import-ipynb) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import-ipynb) (1.15.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.16.2)\n","Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.13.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.11.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->import-ipynb) (3.10.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->import-ipynb) (4.1.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.10.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (22.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.19.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n","Installing collected packages: jedi, import-ipynb\n","Successfully installed import-ipynb-0.1.4 jedi-0.18.1\n","'Copy of ecgGAN.ipynb'\t ecg_GAN        sine_generation\n"," data.py\t\t ecgGAN.ipynb   sin_gen_run\n","'dcgan(2).ipynb'\t mitdb\t        torch_two_sample\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-48f232e08d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdata_sine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msine_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#from data import SineData, PD_to_Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_sine'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["\"\"\"#MMD Evaluation Metric Definition\n","Using MMD to determine the similarity between distributions\n","PDIST code comes from torch-two-sample utils code: \n","    https://github.com/josipd/torch-two-sample/blob/master/torch_two_sample/util.py\n","\"\"\"\n","\n","def pdist(sample_1, sample_2, norm=2, eps=1e-5):\n","    r\"\"\"Compute the matrix of all squared pairwise distances.\n","    Arguments\n","    ---------\n","    sample_1 : torch.Tensor or Variable\n","        The first sample, should be of shape ``(n_1, d)``.\n","    sample_2 : torch.Tensor or Variable\n","        The second sample, should be of shape ``(n_2, d)``.\n","    norm : float\n","        The l_p norm to be used.\n","    Returns\n","    -------\n","    torch.Tensor or Variable\n","        Matrix of shape (n_1, n_2). The [i, j]-th entry is equal to\n","        ``|| sample_1[i, :] - sample_2[j, :] ||_p``.\"\"\"\n","    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n","    norm = float(norm)\n","    \n","    if norm == 2.:\n","        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\n","        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\n","        norms = (norms_1.expand(n_1, n_2) +\n","                 norms_2.transpose(0, 1).expand(n_1, n_2))\n","        distances_squared = norms - 2 * sample_1.mm(sample_2.t())\n","        return torch.sqrt(eps + torch.abs(distances_squared))\n","    else:\n","        dim = sample_1.size(1)\n","        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)\n","        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)\n","        differences = torch.abs(expanded_1 - expanded_2) ** norm\n","        inner = torch.sum(differences, dim=2, keepdim=False)\n","        return (eps + inner) ** (1. / norm)\n","\n","def permutation_test_mat(matrix,\n","                         n_1,  n_2,  n_permutations,\n","                          a00=1,  a11=1,  a01=0):\n","    \"\"\"Compute the p-value of the following statistic (rejects when high)\n","        \\sum_{i,j} a_{\\pi(i), \\pi(j)} matrix[i, j].\n","    \"\"\"\n","    n = n_1 + n_2\n","    pi = np.zeros(n, dtype=np.int8)\n","    pi[n_1:] = 1\n","\n","    larger = 0.\n","    count = 0\n","    \n","    for sample_n in range(1 + n_permutations):\n","        count = 0.\n","        for i in range(n):\n","            for j in range(i, n):\n","                mij = matrix[i, j] + matrix[j, i]\n","                if pi[i] == pi[j] == 0:\n","                    count += a00 * mij\n","                elif pi[i] == pi[j] == 1:\n","                    count += a11 * mij\n","                else:\n","                    count += a01 * mij\n","        if sample_n == 0:\n","            statistic = count\n","        elif statistic <= count:\n","            larger += 1\n","\n","        np.random.shuffle(pi)\n","\n","    return larger / n_permutations\n","\n","\"\"\"Code from Torch-Two-Samples at https://torch-two-sample.readthedocs.io/en/latest/#\"\"\"\n","\n","class MMDStatistic:\n","    r\"\"\"The *unbiased* MMD test of :cite:`gretton2012kernel`.\n","    The kernel used is equal to:\n","    .. math ::\n","        k(x, x') = \\sum_{j=1}^k e^{-\\alpha_j\\|x - x'\\|^2},\n","    for the :math:`\\alpha_j` proved in :py:meth:`~.MMDStatistic.__call__`.\n","    Arguments\n","    ---------\n","    n_1: int\n","        The number of points in the first sample.\n","    n_2: int\n","        The number of points in the second sample.\"\"\"\n","\n","    def __init__(self, n_1, n_2):\n","        self.n_1 = n_1\n","        self.n_2 = n_2\n","\n","        # The three constants used in the test.\n","        self.a00 = 1. / (n_1 * (n_1 - 1))\n","        self.a11 = 1. / (n_2 * (n_2 - 1))\n","        self.a01 = - 1. / (n_1 * n_2)\n","\n","    def __call__(self, sample_1, sample_2, alphas, ret_matrix=False):\n","        r\"\"\"Evaluate the statistic.\n","        The kernel used is\n","        .. math::\n","            k(x, x') = \\sum_{j=1}^k e^{-\\alpha_j \\|x - x'\\|^2},\n","        for the provided ``alphas``.\n","        Arguments\n","        ---------\n","        sample_1: :class:`torch:torch.autograd.Variable`\n","            The first sample, of size ``(n_1, d)``.\n","        sample_2: variable of shape (n_2, d)\n","            The second sample, of size ``(n_2, d)``.\n","        alphas : list of :class:`float`\n","            The kernel parameters.\n","        ret_matrix: bool\n","            If set, the call with also return a second variable.\n","            This variable can be then used to compute a p-value using\n","            :py:meth:`~.MMDStatistic.pval`.\n","        Returns\n","        -------\n","        :class:`float`\n","            The test statistic.\n","        :class:`torch:torch.autograd.Variable`\n","            Returned only if ``ret_matrix`` was set to true.\"\"\"\n","        sample_12 = torch.cat((sample_1, sample_2), 0)\n","        distances = pdist(sample_12, sample_12, norm=2)\n","\n","        kernels = None\n","        for alpha in alphas:\n","            kernels_a = torch.exp(- alpha * distances ** 2)\n","            if kernels is None:\n","                kernels = kernels_a\n","            else:\n","                kernels = kernels + kernels_a\n","\n","        k_1 = kernels[:self.n_1, :self.n_1]\n","        k_2 = kernels[self.n_1:, self.n_1:]\n","        k_12 = kernels[:self.n_1, self.n_1:]\n","\n","        mmd = (2 * self.a01 * k_12.sum() +\n","               self.a00 * (k_1.sum() - torch.trace(k_1)) +\n","               self.a11 * (k_2.sum() - torch.trace(k_2)))\n","        if ret_matrix:\n","            return mmd, kernels\n","        else:\n","            return mmd\n","\n","\n","    def pval(self, distances, n_permutations=1000):\n","        r\"\"\"Compute a p-value using a permutation test.\n","        Arguments\n","        ---------\n","        matrix: :class:`torch:torch.autograd.Variable`\n","            The matrix computed using :py:meth:`~.MMDStatistic.__call__`.\n","        n_permutations: int\n","            The number of random draws from the permutation null.\n","        Returns\n","        -------\n","        float\n","            The estimated p-value.\"\"\"\n","        if isinstance(distances, Variable):\n","            distances = distances.data\n","        return permutation_test_mat(distances.cpu().numpy(),\n","                                    self.n_1, self.n_2,\n","                                    n_permutations,\n","                                    a00=self.a00, a11=self.a11, a01=self.a01)\n","\n","\"\"\"\n","This paper \n","https://arxiv.org/pdf/1611.04488.pdf says that the most common way to \n","calculate sigma is to use the median pairwise distances between the joint data.\n","\"\"\"\n","\n","def pairwisedistances(X,Y,norm=2):\n","    dist = pdist(X,Y,norm)\n","    return np.median(dist.numpy())\n","\n","\n","\"\"\" \n","Function for loading Sine Data \n","\"\"\"\n","\n","def GetSineData(source_file):\n","  compose = transforms.Compose(\n","        [data_sine.PD_to_Tensor()\n","        ])\n","  return data_sine.SineData(source_file ,transform = compose)\n","  \n","  \"\"\"\n","Creating the training set of sine signals\n","\"\"\"\n","\n","source_filename =  '/content/sample_data/sinedata_v2.csv'\n","sine_data = GetSineData(source_file = source_filename)\n","\n","sample_size = 50 #batch size needed for Data Loader and the noise creator function.\n","data_loader = torch.utils.data.DataLoader(sine_data, batch_size=sample_size, shuffle=True)\n","# Num batches\n","num_batches = len(data_loader)\n","\n","\"\"\"Creating the Test Set\"\"\"\n","test_filename =  '/content/sample_data/sinedata_test_v2.csv'\n","\n","sine_data_test = GetSineData(source_file = test_filename)\n","data_loader_test = torch.utils.data.DataLoader(sine_data_test, batch_size=sample_size, shuffle=True)\n","\n","\"\"\"Defining parameters\"\"\"\n","seq_length = sine_data[0].size()[0] #Number of features\n","\n","#Params for the generator\n","hidden_nodes_g = 50\n","layers = 2\n","tanh_layer = False\n","bidir = True\n","\n","#No. of training rounds per epoch\n","D_rounds = 3\n","G_rounds = 1\n","num_epoch = 120\n","learning_rate = 0.0002\n","    \n","#Params for the Discriminator\n","minibatch_layer = 0\n","minibatch_normal_init_ = True\n","num_cvs = 1\n","cv1_out= 10\n","cv1_k = 3\n","cv1_s = 1\n","p1_k = 3\n","p1_s = 2\n","cv2_out = 5\n","cv2_k = 3\n","cv2_s = 1\n","p2_k = 3\n","p2_s = 2\n","\n","\"\"\"# Evaluation of GAN with 1 CNN Layer in Discriminator\n","##Generator and Discriminator training phase\n","\"\"\"\n","\n","minibatch_out = [0,3,5,8,10]\n","for minibatch_layer in minibatch_out:\n","  path = \"/content/drive/MyDrive/Colab Notebooks/sin_gen_run/Run_\"+str(today.strftime(\"%d_%m_%Y\"))+\"_\"+ str(datetime.datetime.now().time()).split('.')[0]\n","  os.mkdir(path)\n","\n","  dict = {'data' : source_filename, \n","          'sample_size' : sample_size, \n","          'seq_length' : seq_length,\n","          'num_layers': layers, \n","          'tanh_layer': tanh_layer,\n","          'bidir': bidir,\n","          'hidden_dims_generator': hidden_nodes_g, \n","          'minibatch_layer': minibatch_layer,\n","          'minibatch_normal_init_' : minibatch_normal_init_,\n","          'num_cvs':num_cvs,\n","          'cv1_out':cv1_out,\n","          'cv1_k':cv1_k,\n","          'cv1_s':cv1_s,\n","          'p1_k':p1_k,\n","          'p1_s':p1_s,\n","          'cv2_out':cv2_out,\n","          'cv2_k':cv2_k,\n","          'cv2_s':cv2_s,\n","          'p2_k':p2_k,\n","          'p2_s':p2_s,\n","          'num_epoch':num_epoch,\n","          'D_rounds': D_rounds,\n","          'G_rounds': G_rounds,  \n","          'learning_rate' : learning_rate\n","         }\n","  #Printing the settings used to file\n","  json = js.dumps(dict)\n","  f = open(path+\"/settings.json\",\"w\")\n","  f.write(json)\n","  f.close()\n","  \n","  #Initialising the generator and discriminator\n","  generator_1 = sine_model.Generator(seq_length,sample_size,hidden_dim =  hidden_nodes_g, tanh_output = tanh_layer, bidirectional = bidir).cuda()\n","  discriminator_1 = sine_model.Discriminator(seq_length, sample_size ,minibatch_normal_init = minibatch_normal_init_, minibatch = minibatch_layer,num_cv = num_cvs, cv1_out = cv1_out,cv1_k = cv1_k, cv1_s = cv1_s, p1_k = p1_k, p1_s = p1_s, cv2_out= cv2_out, cv2_k = cv2_k, cv2_s = cv2_s, p2_k = p2_k, p2_s = p2_s).cuda()\n","  #Loss function \n","  loss_1 = torch.nn.BCELoss()\n","\n","  generator_1.train()\n","  discriminator_1.train()\n","  \n","  #Defining optimizer\n","  d_optimizer_1 = torch.optim.Adam(discriminator_1.parameters(),lr = learning_rate)\n","  g_optimizer_1 = torch.optim.Adam(generator_1.parameters(),lr = learning_rate)\n","\n","  G_losses = []\n","  D_losses = []\n","  mmd_list = []\n","  series_list = np.zeros((1,seq_length))\n","\n","\n","  for n in tqdm(range(num_epoch)):\n","      for n_batch, sample_data in enumerate(data_loader):\n","      \n","        for d in range(D_rounds):\n","          #Train Discriminator on Fake Data\n","          discriminator_1.zero_grad()\n","\n","          h_g = generator_1.init_hidden()\n","\n","          #Generating the noise and label data\n","          noise_sample = Variable(sine_model.noise(len(sample_data),seq_length))\n","\n","          #Use this line if generator outputs hidden states: dis_fake_data, (h_g_n,c_g_n) = generator.forward(noise_sample,h_g)\n","          dis_fake_data = generator_1.forward(noise_sample,h_g).detach()\n","\n","          y_pred_fake = discriminator_1(dis_fake_data)\n","\n","          loss_fake = loss_1(y_pred_fake,torch.zeros([len(sample_data),1]).cuda())\n","          loss_fake.backward()    \n","\n","          #Train Discriminator on Real Data \n","\n","          real_data = Variable(sample_data.float()).cuda()    \n","          y_pred_real  = discriminator_1.forward(real_data)\n","\n","          loss_real = loss_1(y_pred_real,torch.ones([len(sample_data),1]).cuda())\n","          loss_real.backward()\n","\n","          d_optimizer_1.step() #Updating the weights based on the predictions for both real and fake calculations.\n","\n","\n","\n","        #Train Generator  \n","        for g in range(G_rounds):\n","          generator_1.zero_grad()\n","          h_g = generator_1.init_hidden()\n","\n","          noise_sample = Variable(sine_model.noise(len(sample_data), seq_length))\n","\n","\n","          #Use this line if generator outputs hidden states: gen_fake_data, (h_g_n,c_g_n) = generator.forward(noise_sample,h_g)\n","          gen_fake_data = generator_1.forward(noise_sample,h_g)\n","          y_pred_gen = discriminator_1(gen_fake_data)\n","\n","          error_gen = loss_1(y_pred_gen,torch.ones([len(sample_data),1]).cuda())\n","          error_gen.backward()\n","          g_optimizer_1.step()\n","\n","      if (n_batch%100 == 0):\n","          print(\"\\nERRORS FOR EPOCH: \"+str(n)+\"/\"+str(num_epoch)+\", batch_num: \"+str(n_batch)+\"/\"+str(num_batches))\n","          print(\"Discriminator error: \"+str(loss_fake+loss_real))\n","          print(\"Generator error: \"+str(error_gen))\n","      if n_batch ==( num_batches - 1):\n","          G_losses.append(error_gen.item())\n","          D_losses.append((loss_real+loss_fake).item())\n","          \n","          #Saving the parameters of the model to file for each epoch\n","          torch.save(generator_1.state_dict(), path+'/generator_state_'+str(n)+'.pt')\n","          torch.save(discriminator_1.state_dict(),path+ '/discriminator_state_'+str(n)+'.pt')\n","\n","        # Check how the generator is doing by saving G's output on fixed_noise\n","      \n","          with torch.no_grad():\n","              h_g = generator_1.init_hidden()\n","              fake = generator_1(sine_model.noise(len(sample_data), seq_length),h_g).detach().cpu()\n","              generated_sample = torch.zeros(1,seq_length).cuda()\n","              testloader=torch.utils.data.DataLoader(sine_data_test, batch_size=sample_size, shuffle=True)\n","              \n","              \n","              for n_batch, sample_data in enumerate(testloader):\n","                noise_sample_test = sine_model.noise(sample_size, seq_length)\n","                h_g = generator_1.init_hidden()\n","                generated_data = generator_1.forward(noise_sample_test,h_g).detach().squeeze()\n","                generated_sample = torch.cat((generated_sample,generated_data),dim = 0)\n","             \n","              \n","              # Getting the MMD Statistic for each Training Epoch\n","              generated_sample = generated_sample[1:][:]\n","              sigma = [pairwisedistances(sine_data_test[:].type(torch.DoubleTensor),generated_sample.type(torch.DoubleTensor).squeeze())] \n","              mmd = MMDStatistic(len(sine_data_test[:]),generated_sample.size(0))\n","              mmd_eval = mmd(sine_data_test[:].type(torch.DoubleTensor),generated_sample.type(torch.DoubleTensor).squeeze(),sigma, ret_matrix=False)\n","              mmd_list.append(mmd_eval.item())\n","              \n","          \n","          series_list = np.append(series_list,fake[0].numpy().reshape((1,seq_length)),axis=0)\n","          \n","  #Dumping the errors and mmd evaluations for each training epoch.\n","  with open(path+'/generator_losses.txt', 'wb') as fp:\n","      pickle.dump(G_losses, fp)\n","  with open(path+'/discriminator_losses.txt', 'wb') as fp:\n","      pickle.dump(D_losses, fp)   \n","  with open(path+'/mmd_list.txt', 'wb') as fp:\n","      pickle.dump(mmd_list, fp)\n","  \n","  #Plotting the error graph\n","  plt.plot(G_losses,'-r',label='Generator Error')\n","  plt.plot(D_losses, '-b', label = 'Discriminator Error')\n","  plt.title('GAN Errors in Training')\n","  plt.legend()\n","  plt.savefig(path+'/GAN_errors.png')\n","  plt.close()\n","  \n","  \n","  #Plot a figure for each training epoch with the MMD value in the title\n","  i = 0\n","  while i < num_epoch:\n","    if i%3==0:\n","      fig, ax = plt.subplots(3,1,constrained_layout=True)\n","      fig.suptitle(\"Generated fake data\")\n","    for j in range(0,3):\n","      ax[j].plot(series_list[i][:])\n","      ax[j].set_title('Epoch '+str(i)+ ', MMD: %.4f' % (mmd_list[i]))\n","      i = i+1\n","     \n","    plt.savefig(path+'/Training_Epoch_Samples_MMD_'+str(i)+'.png')\n","    plt.close(fig) \n","    \n","\n","  #Checking the diversity of the samples:\n","  generator_1.eval()\n","  h_g = generator_1.init_hidden()\n","  test_noise_sample = sine_model.noise(sample_size, seq_length)\n","  gen_data= generator_1.forward(test_noise_sample,h_g).detach()\n","\n","\n","  plt.title(\"Generated Sine Waves\")\n","  plt.plot(gen_data[random.randint(0,sample_size-1)].tolist(),'-b')\n","  plt.plot(gen_data[random.randint(0,sample_size-1)].tolist(),'-r')\n","  plt.plot(gen_data[random.randint(0,sample_size-1)].tolist(),'-g')\n","  plt.plot(gen_data[random.randint(0,sample_size-1)].tolist(),'-', color = 'orange')\n","  plt.savefig(path+'/Generated_Data_Sample1.png')\n","  plt.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kcTSY4zsQdP","executionInfo":{"status":"ok","timestamp":1668212533163,"user_tz":480,"elapsed":4063997,"user":{"displayName":"CENNET TUGCE TURAN","userId":"11607656021567769031"}},"outputId":"7716233f-4f7d-4fdf-c950-b6e10c1e039b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 120/120 [11:36<00:00,  5.80s/it]\n","sine_model.ipynb:21: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n","100%|██████████| 120/120 [13:14<00:00,  6.62s/it]\n","sine_model.ipynb:21: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n","100%|██████████| 120/120 [13:07<00:00,  6.56s/it]\n","sine_model.ipynb:21: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n","100%|██████████| 120/120 [13:11<00:00,  6.60s/it]\n","sine_model.ipynb:21: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n","100%|██████████| 120/120 [13:12<00:00,  6.60s/it]\n"]}]},{"cell_type":"code","source":["generator_1.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"eNo3RCeRQmbH","executionInfo":{"status":"error","timestamp":1668213305666,"user_tz":480,"elapsed":162,"user":{"displayName":"CENNET TUGCE TURAN","userId":"11607656021567769031"}},"outputId":"2cfb5f8a-cf84-4f80-e096-40ad9df6ac4a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-fd8e7d259162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Generator' object has no attribute 'summary'"]}]},{"cell_type":"code","source":["# Convert Keras model to a tflite model\n","converter = tf.lite.TFLiteConverter.from_saved_model(generator_1)\n","converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n","tflite_model = converter.convert()\n","\n","open(tflite_model_name + '.tflite', 'wb').write(tflite_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"id":"IyTlhyrWPnK4","executionInfo":{"status":"error","timestamp":1668214037853,"user_tz":480,"elapsed":709,"user":{"displayName":"CENNET TUGCE TURAN","userId":"11607656021567769031"}},"outputId":"d3a5de9a-edae-472f-a34a-369bd7aa59fc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-6de4f170e1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert Keras model to a tflite model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZE_FOR_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mfrom_saved_model\u001b[0;34m(cls, saved_model_dir, signature_keys, tags)\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m       \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignature_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m       \u001b[0msignature_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    780\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0mexport_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m   saved_model_proto, debug_info = (\n\u001b[0;32m--> 887\u001b[0;31m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   debug_info_path = file_io.join(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;31m# Build the path to the SavedModel in pbtxt format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   path_to_pbtxt = file_io.join(\n\u001b[0;32m---> 89\u001b[0;31m       \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m       compat.as_bytes(constants.SAVED_MODEL_FILENAME_PBTXT))\n\u001b[1;32m     91\u001b[0m   \u001b[0;31m# Build the path to the SavedModel in pb format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 86\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got Generator(\n  (layer1): LSTM(1, 50, num_layers=2, batch_first=True, bidirectional=True)\n  (out): Linear(in_features=50, out_features=1, bias=True)\n)"]}]}]}