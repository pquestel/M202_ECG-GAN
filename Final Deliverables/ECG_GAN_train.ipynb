{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLBwv1St1gbM",
        "outputId": "d9d6c8f4-8fef-49d4-ca8f-7445431a0e83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODELS_DIR = 'models/'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "    os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = MODELS_DIR + 'model'\n",
        "MODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'\n",
        "MODEL_TFLITE = MODELS_DIR + 'model.tflite'\n",
        "MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'"
      ],
      "metadata": {
        "id": "IC4pLsvDjjxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"#MMD Evaluation Metric Definition\n",
        "Using MMD to determine the similarity between distributions\n",
        "PDIST code comes from torch-two-sample utils code: \n",
        "    https://github.com/josipd/torch-two-sample/blob/master/torch_two_sample/util.py\n",
        "\"\"\"\n",
        "\n",
        "def pdist(sample_1, sample_2, norm=2, eps=1e-5):\n",
        "    r\"\"\"Compute the matrix of all squared pairwise distances.\n",
        "    Arguments\n",
        "    ---------\n",
        "    sample_1 : torch.Tensor or Variable\n",
        "        The first sample, should be of shape ``(n_1, d)``.\n",
        "    sample_2 : torch.Tensor or Variable\n",
        "        The second sample, should be of shape ``(n_2, d)``.\n",
        "    norm : float\n",
        "        The l_p norm to be used.\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor or Variable\n",
        "        Matrix of shape (n_1, n_2). The [i, j]-th entry is equal to\n",
        "        ``|| sample_1[i, :] - sample_2[j, :] ||_p``.\"\"\"\n",
        "    n_1, n_2 = sample_1.size(0), sample_2.size(0)\n",
        "    norm = float(norm)\n",
        "    \n",
        "    if norm == 2.:\n",
        "        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\n",
        "        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\n",
        "        norms = (norms_1.expand(n_1, n_2) +\n",
        "                 norms_2.transpose(0, 1).expand(n_1, n_2))\n",
        "        distances_squared = norms - 2 * sample_1.mm(sample_2.t())\n",
        "        return torch.sqrt(eps + torch.abs(distances_squared))\n",
        "    else:\n",
        "        dim = sample_1.size(1)\n",
        "        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)\n",
        "        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)\n",
        "        differences = torch.abs(expanded_1 - expanded_2) ** norm\n",
        "        inner = torch.sum(differences, dim=2, keepdim=False)\n",
        "        return (eps + inner) ** (1. / norm)\n",
        "\n",
        "def permutation_test_mat(matrix,\n",
        "                         n_1,  n_2,  n_permutations,\n",
        "                          a00=1,  a11=1,  a01=0):\n",
        "    \"\"\"Compute the p-value of the following statistic (rejects when high)\n",
        "        \\sum_{i,j} a_{\\pi(i), \\pi(j)} matrix[i, j].\n",
        "    \"\"\"\n",
        "    n = n_1 + n_2\n",
        "    pi = np.zeros(n, dtype=np.int8)\n",
        "    pi[n_1:] = 1\n",
        "\n",
        "    larger = 0.\n",
        "    count = 0\n",
        "    \n",
        "    for sample_n in range(1 + n_permutations):\n",
        "        count = 0.\n",
        "        for i in range(n):\n",
        "            for j in range(i, n):\n",
        "                mij = matrix[i, j] + matrix[j, i]\n",
        "                if pi[i] == pi[j] == 0:\n",
        "                    count += a00 * mij\n",
        "                elif pi[i] == pi[j] == 1:\n",
        "                    count += a11 * mij\n",
        "                else:\n",
        "                    count += a01 * mij\n",
        "        if sample_n == 0:\n",
        "            statistic = count\n",
        "        elif statistic <= count:\n",
        "            larger += 1\n",
        "\n",
        "        np.random.shuffle(pi)\n",
        "\n",
        "    return larger / n_permutations\n",
        "\n",
        "\"\"\"Code from Torch-Two-Samples at https://torch-two-sample.readthedocs.io/en/latest/#\"\"\"\n",
        "\n",
        "class MMDStatistic:\n",
        "    r\"\"\"The *unbiased* MMD test of :cite:`gretton2012kernel`.\n",
        "    The kernel used is equal to:\n",
        "    .. math ::\n",
        "        k(x, x') = \\sum_{j=1}^k e^{-\\alpha_j\\|x - x'\\|^2},\n",
        "    for the :math:`\\alpha_j` proved in :py:meth:`~.MMDStatistic.__call__`.\n",
        "    Arguments\n",
        "    ---------\n",
        "    n_1: int\n",
        "        The number of points in the first sample.\n",
        "    n_2: int\n",
        "        The number of points in the second sample.\"\"\"\n",
        "\n",
        "    def __init__(self, n_1, n_2):\n",
        "        self.n_1 = n_1\n",
        "        self.n_2 = n_2\n",
        "\n",
        "        # The three constants used in the test.\n",
        "        self.a00 = 1. / (n_1 * (n_1 - 1))\n",
        "        self.a11 = 1. / (n_2 * (n_2 - 1))\n",
        "        self.a01 = - 1. / (n_1 * n_2)\n",
        "\n",
        "    def __call__(self, sample_1, sample_2, alphas, ret_matrix=False):\n",
        "        r\"\"\"Evaluate the statistic.\n",
        "        The kernel used is\n",
        "        .. math::\n",
        "            k(x, x') = \\sum_{j=1}^k e^{-\\alpha_j \\|x - x'\\|^2},\n",
        "        for the provided ``alphas``.\n",
        "        Arguments\n",
        "        ---------\n",
        "        sample_1: :class:`torch:torch.autograd.Variable`\n",
        "            The first sample, of size ``(n_1, d)``.\n",
        "        sample_2: variable of shape (n_2, d)\n",
        "            The second sample, of size ``(n_2, d)``.\n",
        "        alphas : list of :class:`float`\n",
        "            The kernel parameters.\n",
        "        ret_matrix: bool\n",
        "            If set, the call with also return a second variable.\n",
        "            This variable can be then used to compute a p-value using\n",
        "            :py:meth:`~.MMDStatistic.pval`.\n",
        "        Returns\n",
        "        -------\n",
        "        :class:`float`\n",
        "            The test statistic.\n",
        "        :class:`torch:torch.autograd.Variable`\n",
        "            Returned only if ``ret_matrix`` was set to true.\"\"\"\n",
        "        sample_12 = torch.cat((sample_1, sample_2), 0)\n",
        "        distances = pdist(sample_12, sample_12, norm=2)\n",
        "\n",
        "        kernels = None\n",
        "        for alpha in alphas:\n",
        "            kernels_a = torch.exp(- alpha * distances ** 2)\n",
        "            if kernels is None:\n",
        "                kernels = kernels_a\n",
        "            else:\n",
        "                kernels = kernels + kernels_a\n",
        "\n",
        "        k_1 = kernels[:self.n_1, :self.n_1]\n",
        "        k_2 = kernels[self.n_1:, self.n_1:]\n",
        "        k_12 = kernels[:self.n_1, self.n_1:]\n",
        "\n",
        "        mmd = (2 * self.a01 * k_12.sum() +\n",
        "               self.a00 * (k_1.sum() - torch.trace(k_1)) +\n",
        "               self.a11 * (k_2.sum() - torch.trace(k_2)))\n",
        "        if ret_matrix:\n",
        "            return mmd, kernels\n",
        "        else:\n",
        "            return mmd\n",
        "\n",
        "\n",
        "    def pval(self, distances, n_permutations=1000):\n",
        "        r\"\"\"Compute a p-value using a permutation test.\n",
        "        Arguments\n",
        "        ---------\n",
        "        matrix: :class:`torch:torch.autograd.Variable`\n",
        "            The matrix computed using :py:meth:`~.MMDStatistic.__call__`.\n",
        "        n_permutations: int\n",
        "            The number of random draws from the permutation null.\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The estimated p-value.\"\"\"\n",
        "        if isinstance(distances, Variable):\n",
        "            distances = distances.data\n",
        "        return permutation_test_mat(distances.cpu().numpy(),\n",
        "                                    self.n_1, self.n_2,\n",
        "                                    n_permutations,\n",
        "                                    a00=self.a00, a11=self.a11, a01=self.a01)\n",
        "\"\"\"\n",
        "This paper \n",
        "https://arxiv.org/pdf/1611.04488.pdf says that the most common way to \n",
        "calculate sigma is to use the median pairwise distances between the joint data.\n",
        "\"\"\"\n",
        "\n",
        "def pairwisedistances(X,Y,norm=2):\n",
        "    dist = pdist(X,Y,norm)\n",
        "    return np.median(dist.numpy())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ipVScnO-hIvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diSvxk9x2yhK",
        "outputId": "d2c8165f-c855-4a90-e4eb-0047e3634e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "#from torch_two_sample.statistics_diff import MMDStatistic\n",
        "import torch\n",
        "from tensorflow import lite\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "\n",
        "from IPython import display\n",
        "from tqdm import tqdm\n",
        "from shutil import copyfile\n",
        "import pathlib\n",
        "import math\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/drive')\n",
        "\n",
        "\n",
        "norm_value = 1 #2173\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#          Helper functions\n",
        "#################################################################################\n",
        "def test_show(generator, discriminator):\n",
        "    noise = tf.random.normal([1, 125, 50])\n",
        "    generated_ecg = generator(noise, training=False)\n",
        "    print(generated_ecg.shape)\n",
        "    plt.plot(generated_ecg[0, 0, :])\n",
        "    plt.show()\n",
        "    \n",
        "    decision = discriminator(generated_ecg, training=False)\n",
        "    print(decision)\n",
        "    \n",
        "\n",
        "def generate_and_save_ecg(model, epoch, test_input, save):\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(4,3))\n",
        "    plt.plot(predictions[0, 0, :] * norm_value)\n",
        "    # plt.plot(predictions[0, 0, :])\n",
        "    \n",
        "    if save:\n",
        "        plt.savefig('./ecg_at_epoch_{:04d}.png'.format(epoch))\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def prepare_data(dim):\n",
        "#     copyfile(f\"/drive/My Drive/Colab Notebooks/data/fix_signals_400.npy\", \"./fix_signals.npy\")\n",
        "\n",
        "    #Changes Philippe\n",
        "    mat = scipy.io.loadmat(r\"/content/drive/MyDrive/Colab Notebooks/ecgGAN/interp_data_norm.mat\")\n",
        "    \n",
        "    data = mat['interp_data'] \n",
        "    data = np.reshape(data, (data.shape[0],1,data.shape[1]))\n",
        "\n",
        "\n",
        "    #data = np.load('./data/fix_signals_400.npy')\n",
        "    #data = np.reshape(data, (data.shape[0], 1, data.shape[1]))\n",
        "    print('Data shape:', data.shape)\n",
        "\n",
        "\n",
        "    data = data #/ norm_value # Normalize\n",
        "    data = np.array(data, dtype='float32')\n",
        "    data = data[:576]\n",
        "    print(data.shape)\n",
        "    print(np.amax(data))\n",
        "    print(np.amin(data))\n",
        "\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.plot(data[random.randint(0, data.shape[0]-1)][0])\n",
        "    plt.show()\n",
        "\n",
        "    train_size = int(data.shape[0] * 0.9)\n",
        "    test_size = data.shape[0] - train_size\n",
        "    print('test_size,')\n",
        "    print ( test_size)\n",
        "\n",
        "    # Batch and shuffle the data\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(data[:train_size]).shuffle(train_size).batch(BATCH_SIZE)\n",
        "    test_dataset  = tf.data.Dataset.from_tensor_slices(data[train_size:]).shuffle(test_size).batch(1)\n",
        "\n",
        "    seed = tf.random.normal(dim)\n",
        "    \n",
        "    return seed, train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-UNv4JABqTR"
      },
      "outputs": [],
      "source": [
        "#################################################################################\n",
        "#          Testing functions\n",
        "#################################################################################\n",
        "def rmse(targets, predictions):\n",
        "    return np.sqrt(np.mean((targets-predictions)**2))\n",
        "\n",
        "\n",
        "def prd(targets, predictions):\n",
        "    s1 = np.sum((targets-predictions)**2)\n",
        "    s2 = np.sum(targets**2)\n",
        "    return np.sqrt(s1 / s2 * 100)\n",
        "\n",
        "\n",
        "def mmd(targets, predictions):\n",
        "    mmd_stat = MMDStatistic(400, 400)\n",
        "    sample_target = torch.from_numpy(targets.numpy().reshape((400,1)))\n",
        "    sample_pred = torch.from_numpy(predictions.numpy().reshape((400,1)))\n",
        "    \n",
        "    stat = mmd_stat(sample_target, sample_pred, [1.])\n",
        "    return(stat.item())\n",
        "\n",
        "\n",
        "def testing(test_dataset, model, noise_dim):\n",
        "    noise = tf.random.normal(noise_dim)\n",
        "    generated_ecgs = model(noise, training=False)\n",
        "\n",
        "    mmd_sum, prd_sum, rmse_sum = [], [], []\n",
        "    for true_ecg, gen_ecg in zip(test_dataset, generated_ecgs):\n",
        "        prd_sum.append(prd(true_ecg[0][0], gen_ecg[0]))\n",
        "        rmse_sum.append(rmse(true_ecg[0][0], gen_ecg[0]))\n",
        "        mmd_sum.append(mmd(true_ecg[0][0], gen_ecg[0]))\n",
        "\n",
        "    print('mmd :', f'mean={np.mean(mmd_sum):.6f}', f'min={np.min(mmd_sum):.6f}', f'max={np.max(mmd_sum):.6f}')\n",
        "    print('prd :', f'mean={np.mean(prd_sum):.4f}', f'min={np.min(prd_sum):.4f}', f'max={np.max(prd_sum):.4f}')\n",
        "    print('rmse:', f'mean={np.mean(rmse_sum):.4f}', f'min={np.min(rmse_sum):.4f}', f'max={np.max(rmse_sum):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TorJM8KFcGls"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yrhlemc9GIv",
        "outputId": "da7a1763-a460-4cc6-d3ef-0e2d44d70642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 100, 128)          2176      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 100, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 100, 64)           131136    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 100, 64)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d (UpSampling1D) (None, 200, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 200, 32)           32800     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 200, 32)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 200, 16)           8208      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 200, 16)           0         \n",
            "_________________________________________________________________\n",
            "up_sampling1d_1 (UpSampling1 (None, 400, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 400, 1)            257       \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 1, 400)            0         \n",
            "=================================================================\n",
            "Total params: 174,577\n",
            "Trainable params: 174,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 400, 1)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 400, 32)           544       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 400, 32)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 400, 64)           32832     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 400, 64)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 200, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 200, 128)          131200    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 200, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 200, 256)          524544    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25600)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 25601     \n",
            "=================================================================\n",
            "Total params: 714,721\n",
            "Trainable params: 714,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(100, 1)))\n",
        "\n",
        "    #model.add(layers.LSTM(128, time_major=False, return_sequences=True))\n",
        "\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=16, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "  \n",
        "    model.add(layers.Conv1D(filters=32, kernel_size=16, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.UpSampling1D(2))\n",
        "    \n",
        "    model.add(layers.Conv1D(filters=16, kernel_size=16, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Conv1D(filters=8, kernel_size=16, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.UpSampling1D(2))\n",
        "    \n",
        "    model.add(layers.Conv1D(filters=1, kernel_size=16, strides=1, padding='same', activation='tanh'))\n",
        "    \n",
        "    model.add(layers.Permute((2, 1)))\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "   \n",
        "    model.add(layers.Input(shape=(1, 400)))\n",
        "    model.add(layers.Permute((2, 1)))\n",
        "    \n",
        "    model.add(layers.Conv1D(filters=32, kernel_size=16, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # model.add(layers.Dropout(0.4))\n",
        "\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=16, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.MaxPool1D(pool_size=2))\n",
        "\n",
        "    model.add(layers.Conv1D(filters=128, kernel_size=16, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # model.add(layers.Dropout(0.4))\n",
        "\n",
        "    model.add(layers.Conv1D(filters=256, kernel_size=16, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.MaxPool1D(pool_size=2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "generator     = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "    \n",
        "generator.summary()\n",
        "discriminator.summary()\n",
        "\n",
        "# test_show(generator, discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULoCBqer9V_H"
      },
      "outputs": [],
      "source": [
        "#################################################################################\n",
        "#          Prepare metrics for logging\n",
        "#################################################################################\n",
        "\n",
        "# !rm -rf ./logs/\n",
        "\n",
        "### discriminator loss ###\n",
        "disc_log_dir = 'logs/gradient_tape/disc_loss'\n",
        "disc_summary_writer = tf.summary.create_file_writer(disc_log_dir)\n",
        "disc_losses = tf.keras.metrics.Mean('disc_loss', dtype=tf.float32)\n",
        "disc_losses_list = []\n",
        "\n",
        "### discriminator accuracy ###\n",
        "fake_disc_accuracy = tf.keras.metrics.BinaryAccuracy('fake_disc_accuracy')\n",
        "real_disc_accuracy = tf.keras.metrics.BinaryAccuracy('real_disc_accuracy')\n",
        "fake_disc_accuracy_list, real_disc_accuracy_list = [], []\n",
        "\n",
        "### generator loss ###\n",
        "gen_log_dir = 'logs/gradient_tape/gen_loss'\n",
        "gen_summary_writer = tf.summary.create_file_writer(gen_log_dir)\n",
        "gen_losses = tf.keras.metrics.Mean('gen_loss', dtype=tf.float32)\n",
        "gen_losses_list = []\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "#          Prepare loss functions and optimizers\n",
        "#################################################################################\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0002)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u2iNppZ9sku"
      },
      "outputs": [],
      "source": [
        "disc_steps = 5\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_ecg, dim):\n",
        "    noise = tf.random.normal(dim)\n",
        "\n",
        "    for i in range(disc_steps):\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            generated_ecg = generator(noise, training=True)\n",
        "\n",
        "            real_output = discriminator(real_ecg, training=True)\n",
        "            fake_output = discriminator(generated_ecg, training=True)\n",
        "            disc_loss = discriminator_loss(real_output, fake_output)\n",
        "        \n",
        "\n",
        "\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "        \n",
        "        ### for tensorboard ###\n",
        "        disc_losses.update_state(disc_loss)\n",
        "        fake_disc_accuracy.update_state(tf.zeros_like(fake_output), fake_output)\n",
        "        real_disc_accuracy.update_state(tf.ones_like(real_output), real_output)\n",
        "        #######################\n",
        "    \n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        generated_ecg = generator(noise, training=True)\n",
        "        fake_output = discriminator(generated_ecg, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "    \n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    ### for tensorboard ###\n",
        "    gen_losses.update_state(gen_loss)\n",
        "    #######################\n",
        "    \n",
        "\n",
        "def train(dataset, epochs, dim):\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "    \n",
        "        for batch in dataset:\n",
        "            train_step(batch, dim)            \n",
        "        disc_losses_list.append(disc_losses.result().numpy())\n",
        "        gen_losses_list.append(gen_losses.result().numpy())\n",
        "        \n",
        "        fake_disc_accuracy_list.append(fake_disc_accuracy.result().numpy())\n",
        "        real_disc_accuracy_list.append(real_disc_accuracy.result().numpy())\n",
        "        \n",
        "        ## for tensorboard ###\n",
        "        with disc_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', disc_losses.result(), step=epoch)\n",
        "            tf.summary.scalar('fake_accuracy', fake_disc_accuracy.result(), step=epoch)\n",
        "            tf.summary.scalar('real_accuracy', real_disc_accuracy.result(), step=epoch)\n",
        "            \n",
        "        with gen_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', gen_losses.result(), step=epoch)\n",
        "            \n",
        "        disc_losses.reset_states()        \n",
        "        gen_losses.reset_states()\n",
        "        \n",
        "        fake_disc_accuracy.reset_states()\n",
        "        real_disc_accuracy.reset_states()\n",
        "        #######################\n",
        "        generate_and_save_ecg(generator, epochs, seed, False)\n",
        "        generate_and_save_ecg(generator, epochs, seed, False)\n",
        "        generate_and_save_ecg(generator, epochs, seed, False)\n",
        "        # Save the model every 5 epochs\n",
        "#         if (epoch + 1) % 5 == 0:\n",
        "#             generate_and_save_ecg(generator, epochs, seed, False)\n",
        "#             checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    # Generate after the final epoch\n",
        "    #display.clear_output(wait=True)\n",
        "    generate_and_save_ecg(generator, epochs, seed, False)\n",
        "    testing(test_dataset, generator, [57, 100, 1])\n",
        "    \n",
        "#     generator.save('/content/drive/My Drive/Colab Notebooks/saved_models/generator_1500.h5')\n",
        "#     discriminator.save('/content/drive/My Drive/Colab Notebooks/saved_models/discriminator_1500.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "w3o7ZlWOK1Hg",
        "outputId": "77790593-4dca-48a9-991d-8172aa39c0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (565, 1, 400)\n",
            "(565, 1, 400)\n",
            "1.0\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_size,\n",
            "57\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "noise_dim = [BATCH_SIZE, 100, 1]\n",
        "\n",
        "seed, train_dataset, test_dataset = prepare_data(noise_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ubIM_7zrSYah"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx8xSU1v929d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "bd6d379b-af13-4ed3-edef-7c2f46e55bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/100 [00:47<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-07688e7c6562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-4a22f98d0f10>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs, dim)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdisc_losses_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mgen_losses_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(train_dataset, 100, noise_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt08T3oUvue7"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir logs/gradient_tape\n",
        "\n",
        "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes[0].set_ylabel(\"Losses\", fontsize=14)\n",
        "axes[0].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[0].plot(disc_losses_list, color='red')\n",
        "axes[0].plot(gen_losses_list, color='blue')\n",
        "\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[1].plot(fake_disc_accuracy_list, color='red', label = 'fake')\n",
        "axes[1].plot(real_disc_accuracy_list, color='blue')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "5ZUX9d2tSfbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk_e-Ke8Fbm_"
      },
      "outputs": [],
      "source": [
        "mat = scipy.io.loadmat(r\"/content/drive/MyDrive/Colab Notebooks/ecgGAN/interp_data_norm.mat\")\n",
        "\n",
        "data = mat['interp_data'] \n",
        "data = np.reshape(data, (data.shape[0],1,data.shape[1]))\n",
        "\n",
        "\n",
        "#data = np.load('./data/fix_signals_400.npy')\n",
        "#data = np.reshape(data, (data.shape[0], 1, data.shape[1]))\n",
        "print('Data shape:', data.shape)\n",
        "\n",
        "\n",
        "data = data #/ norm_value # Normalize\n",
        "data = np.array(data, dtype='float32')\n",
        "\n",
        "for i in range(20):\n",
        "  seed = tf.random.normal(noise_dim)\n",
        "  ecg = generator(seed, training=False)\n",
        "  answer = discriminator(ecg)\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "  ax1.plot(ecg[0, 0, :] * norm_value)\n",
        "  ax2.plot(data[i][0])\n",
        "  plt.show()\n",
        "  print(answer[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-xX3W_-dwN0"
      },
      "outputs": [],
      "source": [
        "generator.save('/drive/My Drive/Colab Notebooks/saved_models/generator_80e.h5')\n",
        "discriminator.save('/drive/My Drive/Colab Notebooks/saved_models/discriminator_80e.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E3dDE7JMvm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3d3bd5-7921-4f06-e339-dced69871ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/ecgGAN/generator_80e.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "model_no_quant_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(MODEL_TFLITE, \"wb\").write(model_no_quant_tflite)\n",
        "\n"
      ],
      "metadata": {
        "id": "Qyw9GkjzeQC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_size(file_path):\n",
        "    size = os.path.getsize(file_path)\n",
        "    return size\n",
        "def convert_bytes(size, unit=None):\n",
        "    if unit == \"KB\":\n",
        "        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
        "    elif unit == \"MB\":\n",
        "        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
        "    else:\n",
        "        return print('File size: ' + str(size) + ' bytes')\n",
        "convert_bytes(get_file_size('/content/models/model_no_quant.tflite'), \"KB\")\n",
        "convert_bytes(get_file_size('/content/drive/MyDrive/Colab Notebooks/ecgGAN/generator_80e.h5'), \"KB\")"
      ],
      "metadata": {
        "id": "-mzn2NDjO6j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJXJuGLFGfjB"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path = MODEL_NO_QUANT_TFLITE)\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(\"Input Shape:\", input_details[0]['shape'])\n",
        "print(\"Input Type:\", input_details[0]['dtype'])\n",
        "print(\"Output Shape:\", output_details[0]['shape'])\n",
        "print(\"Output Type:\", output_details[0]['dtype'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ASzOhS9R5on"
      },
      "outputs": [],
      "source": [
        "#if you want to save the TF Lite model use below steps or else skip\n",
        "tflite_model_files = pathlib.Path('/tmp/pretrainedmodel.tflite')\n",
        "tflite_model_files.write_bytes(MODEL_TFLITE)\n",
        "# Load TFLite model using interpreter and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_content=MODEL_TFLITE)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function: Convert some hex value into an array for C programming\n",
        "def hex_to_c_array(hex_data, var_name):\n",
        "\n",
        "  c_str = ''\n",
        "\n",
        "  # Create header guard\n",
        "  c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
        "  c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
        "\n",
        "  # Add array length at top of file\n",
        "  c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
        "\n",
        "  # Declare C variable\n",
        "  c_str += 'unsigned char ' + var_name + '[] = {'\n",
        "  hex_array = []\n",
        "  for i, val in enumerate(hex_data) :\n",
        "\n",
        "    # Construct string from hex\n",
        "    hex_str = format(val, '#04x')\n",
        "\n",
        "    # Add formatting so each line stays within 80 characters\n",
        "    if (i + 1) < len(hex_data):\n",
        "      hex_str += ','\n",
        "    if (i + 1) % 12 == 0:\n",
        "      hex_str += '\\n '\n",
        "    hex_array.append(hex_str)\n",
        "\n",
        "  # Add closing brace\n",
        "  c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
        "\n",
        "  # Close out header guard\n",
        "  c_str += '#endif //' + var_name.upper() + '_H'\n",
        "\n",
        "  return c_str\n",
        "\n"
      ],
      "metadata": {
        "id": "Nkt47C_CPpjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Write TFLite model to a C source (or header) file\n",
        "with open( 'ecg_model.h', 'w') as file:\n",
        "  file.write(hex_to_c_array(MODEL_TFLITE, 'ecg_model.h'))\n",
        "\n"
      ],
      "metadata": {
        "id": "chqc_1JGPqMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aIoDPI5SI5-"
      },
      "outputs": [],
      "source": [
        "seed = tf.random.normal([1, 100, 1])\n",
        "ecg = model(seed, training=False)\n",
        "fig = plt.figure(figsize=(4,3))\n",
        "plt.plot(ecg[0, 0, :] * norm_value)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm9jiDCyaQYP"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/ecgGAN/_old.h5')\n",
        "testing(test_dataset, model, [57, 100, 12])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6HwS7tGPKFQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKlg0Fwlk6dU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}